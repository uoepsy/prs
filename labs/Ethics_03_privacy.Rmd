---
title: "Model fit and standardization"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---


```{r setup, include=FALSE}
source('assets/setup.R')

# knitr::opts_chunk$set(cache = TRUE)
set.seed(1)
```

:::green
Be sure to check the [**solutions to last week's exercises**](02_slr.html).<br>You can still ask any questions about previous weeks' materials if things aren't clear!
:::

:::lo
**LEARNING OBJECTIVES**

1. Understand the calculation and interpretation of the coefficient of determination. 
1. Understand the calculation and interpretation of the F-test of model utility.
1. Understand how to standardize model coefficients and when this is appropriate to do.
1. Understand the relationship between the correlation coefficient and the regression slope.
:::



# Data recap

`r optbegin('Data: riverview.csv. Click the plus to expand &#8594;', FALSE, show = TRUE, toggle = params$TOGGLE)`
**Download link**

[Download the data here](https://uoepsy.github.io/data/riverview.csv){target="_blank"}

**Description**

The riverview data come from @Lewis-Beck2015 and contain five attributes collected from a random sample of $n=32$ employees working for the city of Riverview, a hypothetical midwestern city in the US. The attributes include:

- `education`: Years of formal education
- `income`: Annual income (in thousands of U.S. dollars)
- `seniority`: Years of seniority
- `gender`: Employee's gender
- `male`: Dummy coded gender variable (0 = Female, 1 = Male)
- `party`: Political party affiliation


**Preview**

The first six rows of the data are:

```{r echo=FALSE}
library(tidyverse)
library(kableExtra)

riverview <- read_csv('https://uoepsy.github.io/data/riverview.csv')
kable(head(riverview), align='c') %>% kable_styling(full_width = FALSE)
```
`r optend()`


`r qbegin(1)`
Read the riverview data from the previous lab into R, and fit a linear model to investigate how income varies with years of formal education.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
library(tidyverse)

riverview <- read_csv(file = "https://uoepsy.github.io/data/riverview.csv")
head(riverview)

mdl <- lm(income ~ 1 + education, data = riverview)
mdl
```

The fitted model is:
$$
\widehat{Income} = 11.32 + 2.65 \times Education
$$
`r solend()`



# Partitioning variation

We might ask ourselves if the model is useful. To quantify and assess model utility, we split the total variability of the response into two terms: the variability explained by the model plus the variability left unexplained in the residuals.

$$
\text{total variability in response = variability explained by model + unexplained variability in residuals}
$$

Each term is quantified by a sum of squares:

$$
\begin{aligned}
SS_{Total} &= SS_{Model} + SS_{Residual} \\
\sum_{i=1}^n (y_i - \bar y)^2 &= \sum_{i=1}^n (\hat y_i - \bar y)^2 + \sum_{i=1}^n (y_i - \hat y_i)^2
\end{aligned}
$$


`r qbegin(2)`
What is the proportion of the total variability in incomes explained by the linear relationship with education level?

_**Hint:** The question asks to compute the value of $R^2$._
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
The proportion of the total variability in incomes explained by the linear relationship with education level is given by R-squared.

**Option 1**

The R-squared coefficient is defined as:
$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$

In R we can write:
```{r}
riverview_fitted <- riverview %>%
  mutate(
    income_hat = predict(mdl),
    resid = income - income_hat
  )
head(riverview_fitted)

riverview_fitted %>%
  summarise(
    SSModel = sum( (income_hat - mean(income))^2 ),
    SSTotal = sum( (income - mean(income))^2 )
  ) %>%
  summarise(
    RSquared = SSModel / SSTotal
  )
```


---

**Option 2**

```{r}
summary(mdl)
```

The output of `summary()` displays the R-squared value in the following line:
```
Multiple R-squared:  0.6317
```

For the moment, ignore "Adjusted R-squared". We will come back to this later in the course.

---

**Option 3**

We can perform an **AN**alysis **O**f **VA**riance or, in short, ANOVA.
It simply means that we are examining/partitioning the total variability of a response variable.

The `anova()` function returns the sum of squares of interest in the column `Sum Sq`:
```{r}
mdl_anova <- anova(mdl)
mdl_anova
```

Consider the column `Sum Sq`. 
The entry corresponding to `education` gives $SS_{Model}$ = 4147.3, as education is the explanatory variable. The entry corresponding to `Residuals` gives $SS_{Residual}$ = 2418.2.

```{r}
# Because the column name Sum Sq has a space, 
# we need to wrap it with backticks
SSModel <- mdl_anova$`Sum Sq`[1]
SSResidual <- mdl_anova$`Sum Sq`[2]
SSTotal <- SSModel + SSResidual

RSquared <- SSModel / SSTotal
RSquared
```

---

**Interpretation**

:::int
Approximately 63\% of the total variability in employee incomes is explained by the linear association with education level.
:::

`r solend()`



# Model utility test

To test if the model is useful --- that is, if the explanatory variable is a useful predictor of the response --- we test the following hypotheses:

$$
\begin{aligned}
H_0 &: \text{the model is ineffective, } \beta_1 = 0 \\
H_1 &: \text{the model is effective, } \beta_1 \neq 0
\end{aligned}
$$


The relevant test-statistic is the F-statistic:

$$
\begin{split}
F = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model} / 1}{SS_{Residual} / (n-2)}
\end{split}
$$

which compares the amount of variation in the response explained by the model to the amount of variation left unexplained in the residuals.

The sample F-statistic is compared to an F-distribution with $df_{1} = 1$ and $df_{2} = n - 2$ degrees of freedom.^[
$SS_{Total}$ has $n - 1$ degrees of freedom as one degree of freedom is lost in estimating the population mean with the sample mean $\bar{y}$.
$SS_{Residual}$ has $n - 2$ degrees of freedom. There are $n$ residuals, but two degrees of freedom are lost in estimating the intercept and slope of the line used to obtain the $\hat y_i$s.
Hence, by difference, $SS_{Model}$ has $n - 1 - (n - 2) = 1$ degree of freedom.
]


`r qbegin(3)`
Perform a model utility test at the 5\% significance level, by computing the F-statistic using its definition.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
df1 <- 1
df2 <- nrow(riverview) - 2
f_star <- qf(0.95, df1, df2)
f_star
```

```{r}
model_utility <- riverview_fitted %>%
  summarise(
    SSModel = sum( (income_hat - mean(income))^2 ),
    SSResid = sum( resid^2 ),
    MSModel = SSModel / 1,
    MSResid = SSResid / df2,
    FObs = MSModel / MSResid
  )
model_utility
```


:::int
We performed an F-test of model utility at the 5\% significance level, where $F(1,30) = 51.45$.

As the observed $F = 51.45$ is much larger than the critical value $F^* = 4.17$, we have strong evidence to reject the null hypothesis that the model is ineffective.
:::

<br>
Alternatively, we can compute the p-value:
```{r}
pvalue <- 1 - pf(model_utility$FObs, df1, df2)
pvalue
```
The value `5.562116e-08` simply means $5.56 \times 10^{-8}$, so it's a really small number.

:::int
We performed an F-test of model utility at the 5\% significance level, where $F(1,30) = 51.45, p<.001$.

The p-value (< .001) is much lower than the specified significance level, meaning that we have very strong evidence against  the null hypothesis.
:::

`r solend()`


`r optbegin('Optional: Another formula for the F-test. Click the plus to expand &#8594;', FALSE)`
With some algebra we can also show that:
$$
F = \frac{R^2 / 1}{(1 - R^2) / (n - 2) } = \frac{R^2 / df_{Model}}{(1 - R^2) / df_{Residual} }
$$

Proof:

$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\frac{SS_{Model}}{SS_{Total}}}{\frac{SS_{Residual}}{SS_{Total}} \cdot \frac{1}{(n - 2)}} 
= \frac{R^2 / 1}{(1 - R^2) / (n - 2)}
\end{aligned}
$$
`r optend()`

`r qbegin(4)`
Look at the output of `summary(mdl)` and `anova(mdl)`.

For each output, identify the relevant information to conduct an F-test against the null hypothesis that the model is ineffective at predicting income using education level.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
summary(mdl)
```

The relevant row is the following:
```
F-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08
```

---

Instead, the `anova()` output is:
```{r}
anova(mdl)
```

And the relevant entries are:

- the `Df` column, which contains the degrees of freedom;
- `F value = 51.452`, which is the F-statistic;
- `Pr(>F) = 5.562e-08` = $5.562 \times 10^{-8}$, which is the p-value.


---

We might write up the test results as,

:::int
We performed an F-test for the overall significance of the regression, $F(1, 30) = 51.45, p < .001$.
The large F-statistic leads to a very small p-value ($<.001$), meaning that we have very strong evidence against the null hypothesis that the model is ineffective.

In other words, the data provide strong evidence that education is an effective predictor of income.
:::
`r solend()`


`r qbegin(5)`
Consider the `F value` output of `anova(mdl)` and the `t value` for education returned by `summary(mdl)`

```
F value = 51.452
t value = 7.173
```

Do you notice any relationship between the F-statistic for overall model utility and the t-statistic for $H_0: \beta_1 = 0$?
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
**In simple linear regression only**, the F-statistic for overall model significance is equal to the square of the t-statistic for $H_0: \beta_1 = 0$.

You can check that the squared t-statistic is equal, up to rounding error, to the F-statistic:
$$
t^2 = F \\
7.173^2 = 51.452
$$

`r optbegin('Optional: Equivalence of t-test for the slope and model utility F-test in SLR. Click the plus to expand &#8594;', FALSE)`

Here we will show the equivalence of the F-test for model effectiveness and t-test for the slope.

Recall the formula of the sum of squares due to the model. We will rewrite it in an equivalent form below:
$$
\begin{aligned}
SS_{Model} &= \sum_i (\hat y_i - \bar y)^2 \\
&= \sum_i (\hat \beta_0 + \hat \beta_1 x_i - \bar y)^2 \\
&= \sum_i (\bar y - \hat \beta_1 \bar x + \hat \beta_1 x_i - \bar y)^2 \\
&= \sum_i (\hat \beta_1 (x_i - \bar x))^2 \\
&= \hat \beta_1^2 \sum_i (x_i - \bar x)^2
\end{aligned}
$$

The F-statistic is given by:
$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\hat \beta_1^2 \sum_i (x_i - \bar x)^2}{\hat \sigma^2} 
= \frac{\hat \beta_1^2 }{\hat \sigma^2 / \sum_i (x_i - \bar x)^2}
\end{aligned}
$$

Now recall the formula of the t-statistic,
$$
t = \frac{\hat \beta_1}{SE(\hat \beta_1)} = \frac{\hat \beta_1}{\hat \sigma / \sqrt{\sum_i (x_i - \bar x)^2}}
$$

It is evident that the latter is obtained as the square root of the former.

`r optend()`

`r solend()`


# Back to regression coefficients

`r qbegin(6)`
Compute the average education level and the average income in the sample.

Use the `predict()` function to compute the predicted income for those with average education level.

What do you notice?
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
Sample means:
```{r}
stats <- riverview %>%
  summarise(
    mean_education = mean(education),
    mean_income = mean(income)
  )
stats
```

Prediction:
```{r}
query <- tibble(education = stats$mean_education)
query

predict(mdl, newdata = query)
```

<br>
The predicted average income for those having average education level is equal to the average income in the sample.
`r solend()`



`r qbegin(7)`
Let's formalise the previous question using symbols.
Consider the fitted model $\hat{y} = \hat \beta_0 + \hat \beta_1 x$.

What is the predicted response for an individual having an explanatory variable at the average level $\bar{x}$?

_**Hint:** Substitute the formula of $\hat \beta_0$ into the equation of the fitted model._
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
We require the prediction at $x = \bar{x}$, that is:
$$
\hat{y} = \hat \beta_0 + \hat \beta_1 \bar{x}
$$
Recall the formula for the fitted intercept: $\hat \beta_0 = \bar{y} - \hat \beta_1 \bar{x}$. Let's substitute it into the equation:

$$
\begin{split}
\hat{y} &= \hat \beta_0 + \hat \beta_1 \bar{x} \\
&= \bar{y} - \hat \beta_1 \bar{x} + \hat \beta_1 \bar{x} \\
&= \bar{y}
\end{split}
$$

This highlights an important property of the fitted regression line: it always passes through the point of averages $(\bar x, \bar y)$.

Intuitively, what would be your prediction $\hat y$ when $x$ equals $\bar{x}$? If you guessed $\bar{y}$ you're on track!

`r solend()`


# Standardization

`r qbegin(8)`
Add to the riverview dataset two variables called `z_education` and `z_income` representing the standardized education and income variables, respectively.

**Without using R**, if you were to fit a linear regression model using the standardized response and standardized predictor, what would the intercept be?

<br>
_**Hint:** Recall the formula for the $z$-score:_
$$
z_x = \frac{x - \bar{x}}{s_x}, \qquad z_y = \frac{y - \bar{y}}{s_y}
$$
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
riverview <- riverview %>%
  mutate(
    z_education = (education - mean(education)) / sd(education),
    z_income = (income - mean(income)) / sd(income)
  )
```

Check that they are standardized:
```{r}
riverview %>%
  summarise(
    M_z_education = mean(z_education), SD_z_education = sd(z_education), 
    M_z_income = mean(z_income), SD_z_income = sd(z_income)
  )
```

A standardized variable has mean 0 and standard deviation equal to 1.
We can substitute this in the formula of the estimated intercept:

$$
\hat \beta_0 = \bar{y} - \hat \beta_1 \bar{x} = 0 - \hat \beta_1 0 = 0
$$

The intercept of the linear model between the standardized variables is zero --- that is, the regression line passes through the origin of the graph. 

But wait... The origin is also the average of $z_x$ and the average of $z_y$ as they are standardized. In symbols $(0, 0) = (\bar{z}_x, \bar{z}_y)$. 

We expected the fitted line to pass through the point of averages --- in this case, the origin.
`r solend()`


`r qbegin(9)`
Using R, fit the regression model using the standardized response and explanatory variables.

What is the slope equal to?
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
Linear model for standardized variables:
```{r}
mdl_z <- lm(z_income ~ 1 + z_education, data = riverview)
summary(mdl_z)
```

<br>
Correlation coefficient of original variables:
```{r}
riverview %>%
  select(education, income) %>%
  cor()
```

<br>
The slope of the standardized variables is equal to the correlation between the original variables.
`r solend()`


`r qbegin(10)`
Interpret the slope of the standardized variables.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
:::int
For every standard deviation increase in education, income increases on average by 0.79 standard deviations.
:::
`r solend()`




# References

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
